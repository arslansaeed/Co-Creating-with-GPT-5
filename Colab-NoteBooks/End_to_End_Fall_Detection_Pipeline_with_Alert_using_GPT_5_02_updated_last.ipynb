{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Installations"
      ],
      "metadata": {
        "id": "nv96an_SwnAO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyfHzQUNXgnG"
      },
      "outputs": [],
      "source": [
        "# Import requirements\n",
        "# Core scientific + visualization libraries\n",
        "!pip install numpy matplotlib pillow\n",
        "\n",
        "# Computer vision\n",
        "!pip install opencv-python\n",
        "\n",
        "# Roboflow client + inference SDK\n",
        "!pip install roboflow inference\n",
        "\n",
        "# Supervision library (for annotations, tracking, etc.)\n",
        "!pip install supervision==0.19.0\n",
        "\n",
        "# Optional: Jupyter/Colab utilities\n",
        "!pip install ipython\n",
        "\n",
        "!pip install gtts\n",
        "\n",
        "!pip -q install ultralytics opencv-python timm gTTS groq langgraph pydantic==2.* torch torchvision\n",
        "\n",
        "!pip install supervision\n",
        "\n",
        "\n",
        "# Import needed modules\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "import supervision as sv\n",
        "import numpy as np\n",
        "import shutil\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "\n",
        "from inference_sdk import InferenceHTTPClient\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image as im\n",
        "\n",
        "!pip install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Keys Setting"
      ],
      "metadata": {
        "id": "OVb11iUywr1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVWP5qBxXhVR"
      },
      "outputs": [],
      "source": [
        "# Run this cell BEFORE the workflow that reads os.environ[...]\n",
        "\n",
        "ROBOFLOW_API_KEY = \"\"      # e.g., rf_...\n",
        "GROQ_API_KEY     = \"\"          # e.g., gsk_...\n",
        "EMAIL_USER       = \"\"      # e.g., you@gmail.com\n",
        "EMAIL_PASS       = \"\"\n",
        "CAREGIVER_EMAIL  = \"\"\n",
        "\n",
        "# Export to environment so your existing code (which uses os.environ) keeps working unchanged\n",
        "import os\n",
        "os.environ[\"ROBOFLOW_API_KEY\"] = ROBOFLOW_API_KEY\n",
        "os.environ[\"GROQ_API_KEY\"]     = GROQ_API_KEY\n",
        "os.environ[\"EMAIL_USER\"]       = EMAIL_USER\n",
        "os.environ[\"EMAIL_PASS\"]       = EMAIL_PASS\n",
        "os.environ[\"CAREGIVER_EMAIL\"]  = CAREGIVER_EMAIL\n",
        "\n",
        "# Quick asserts (optional)\n",
        "assert os.environ[\"ROBOFLOW_API_KEY\"]\n",
        "assert os.environ[\"GROQ_API_KEY\"]\n",
        "assert os.environ[\"EMAIL_USER\"] and os.environ[\"EMAIL_PASS\"] and os.environ[\"CAREGIVER_EMAIL\"]\n",
        "print(\"âœ… Hard-coded keys loaded into environment.\")\n",
        "\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "\n",
        "OPENAI_BASE_URL = os.environ.get(\"OPENAI_BASE_URL\", \"https://api.aimlapi.com/v1\")\n",
        "\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\").strip()\n",
        "assert OPENAI_API_KEY, \"âŒ OPENAI_API_KEY missing.\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§­ Agentic Workflow (Video â†’ Roboflow â†’ GPT-5 â†’ Email)\n",
        "\n",
        "ðŸŽ¥ **Video Stream** âž¡ï¸ ðŸ§¿ **Roboflow (CV Annotation)** âž¡ï¸ ðŸ§  **GPT-5 (Fall Detection & Reasoning)** âž¡ï¸ ðŸ“§ **Email Caregiver (Alert)**"
      ],
      "metadata": {
        "id": "SuUeNaTxRs9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LangGraph + Roboflow + GPT-5 + Email Alerts"
      ],
      "metadata": {
        "id": "yT2wcPnTxKVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Agentic Fall Monitoring (LangGraph)\n",
        "# - Node 1: Roboflow annotate + ByteTrack + timers  â†’ vision.mp4\n",
        "# - Node 2: Groq fall scan on vision.mp4            â†’ voice + email + JSON\n",
        "# =========================================================\n",
        "\n",
        "\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, cv2, base64, mimetypes, json, time, shutil, ssl, smtplib\n",
        "import numpy as np\n",
        "from typing import Dict, Optional, Tuple, Any, List\n",
        "from PIL import Image as PILImage\n",
        "from IPython.display import display, Audio, Video\n",
        "import supervision as sv\n",
        "from inference_sdk import InferenceHTTPClient\n",
        "from groq import Groq\n",
        "from gtts import gTTS\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.base import MIMEBase\n",
        "from email import encoders\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# =========================================================\n",
        "#                       CONFIG\n",
        "# =========================================================\n",
        "# Paths\n",
        "INPUT_VIDEO         = \"/content/sample_detection.mp4\"        # original (downloaded if missing)\n",
        "FRAMES_DIR          = \"/content/frames\"\n",
        "ANNOTATED_DIR       = \"/content/annotated\"\n",
        "ANNOTATED_VIDEO     = \"/content/vision.mp4\"                  # output of Node 1\n",
        "GROQ_REPORT_PATH    = \"/content/groq_fall_report.json\"\n",
        "TRIGGER_FRAME_PATH  = \"/content/fall_detected.jpg\"\n",
        "TMP_JPEG_PATH       = \"/content/_groq_frame.jpg\"\n",
        "\n",
        "# Roboflow & model\n",
        "MODEL_ID = \"fall-detection-real/1\"   # classes: standing / falling / fallen\n",
        "\n",
        "# Model choices\n",
        "OPENAI_MODEL = \"openai/gpt-5-chat-latest\"\n",
        "GROQ_MODEL   = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
        "\n",
        "# Prompts (unchanged)\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You analyze annotated video frames for fall detection. \"\n",
        "    \"Use posture and floor contact primarily; labels may be small or missing. \"\n",
        "    \"If any person appears fallen, reply exactly: 'A person has fallen'. \"\n",
        "    \"Otherwise reply exactly: 'No fall detected'.\"\n",
        ")\n",
        "USER_PROMPT = \"Strict fall check. Reply EXACTLY one of the two phrases.\"\n",
        "\n",
        "\n",
        "# Timers / thresholds\n",
        "FALL_SECONDS_THRESHOLD = 30   # prolonged fallen time for model-based (Roboflow) check\n",
        "TARGET_SAMPLE_SEC      = 0.25 # Groq sampling cadence (~0.25s)\n",
        "HEARTBEAT_VOICE        = True\n",
        "HEARTBEAT_EVERY_SEC    = 10\n",
        "PLAY_AUDIO_ON_DETECT   = True\n",
        "\n",
        "# Define IN_COLAB\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "#                 ENV / Keys (assertions)\n",
        "# =========================================================\n",
        "ROBOFLOW_API_KEY = os.environ.get(\"ROBOFLOW_API_KEY\",\"\").strip()\n",
        "GROQ_API_KEY     = os.environ.get(\"GROQ_API_KEY\",\"\").strip()\n",
        "EMAIL_USER       = os.environ.get(\"EMAIL_USER\",\"\").strip()\n",
        "EMAIL_PASS       = os.environ.get(\"EMAIL_PASS\",\"\").strip()\n",
        "CAREGIVER_EMAIL  = os.environ.get(\"CAREGIVER_EMAIL\",\"\").strip()\n",
        "\n",
        "assert ROBOFLOW_API_KEY, \"âŒ ROBOFLOW_API_KEY missing.\"\n",
        "assert GROQ_API_KEY,     \"âŒ GROQ_API_KEY missing.\"\n",
        "assert EMAIL_USER and EMAIL_PASS and CAREGIVER_EMAIL, \"âŒ EMAIL_USER/EMAIL_PASS/CAREGIVER_EMAIL missing.\""
      ],
      "metadata": {
        "id": "_RUtU9Fqxpqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LangGraph State & Utility Functions"
      ],
      "metadata": {
        "id": "Lyua1dktzplN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZHxlyT0YJ1M"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "#               STATE (Pydantic) for LangGraph\n",
        "# =========================================================\n",
        "class AssistState(BaseModel):\n",
        "    input_video: str = Field(default=INPUT_VIDEO)\n",
        "    annotated_video: str = Field(default=ANNOTATED_VIDEO)\n",
        "    frames_dir: str = Field(default=FRAMES_DIR)\n",
        "    annotated_dir: str = Field(default=ANNOTATED_DIR)\n",
        "    # Stage 1 outputs\n",
        "    num_frames: int = 0\n",
        "    video_fps: float = 0.0\n",
        "    prolonged_fall_ids: List[int] = Field(default_factory=list)\n",
        "    # Stage 2 outputs\n",
        "    groq_confirmed: bool = False\n",
        "    groq_first_detection_frame: Optional[int] = None\n",
        "    groq_first_detection_time_s: Optional[float] = None\n",
        "    trigger_frame_path: Optional[str] = None\n",
        "    groq_report_path: str = Field(default=GROQ_REPORT_PATH)\n",
        "    # Internal logging\n",
        "    logs: List[str] = Field(default_factory=list)\n",
        "\n",
        "# =========================================================\n",
        "#                     UTIL HELPERS\n",
        "# =========================================================\n",
        "def log(state: AssistState, msg: str) -> None:\n",
        "    print(msg)\n",
        "    state.logs.append(msg)\n",
        "\n",
        "def speak_once(text=\"Fall detected.\", path=\"/content/fall_alert.mp3\"):\n",
        "    if not PLAY_AUDIO_ON_DETECT:\n",
        "        return\n",
        "    try:\n",
        "        gTTS(text=text, lang=\"en\").save(path)\n",
        "        display(Audio(path, autoplay=True))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def send_email_alert(subject: str, body: str, to_email: str, attach_path: Optional[str] = None):\n",
        "    msg = MIMEMultipart()\n",
        "    msg[\"From\"] = EMAIL_USER\n",
        "    msg[\"To\"] = to_email\n",
        "    msg[\"Subject\"] = subject\n",
        "    msg.attach(MIMEText(body, \"plain\"))\n",
        "    if attach_path and os.path.exists(attach_path):\n",
        "        with open(attach_path, \"rb\") as f:\n",
        "            part = MIMEBase(\"application\", \"octet-stream\")\n",
        "            part.set_payload(f.read())\n",
        "        encoders.encode_base64(part)\n",
        "        part.add_header(\"Content-Disposition\", f'attachment; filename=\"{os.path.basename(attach_path)}\"')\n",
        "        msg.attach(part)\n",
        "\n",
        "    context = ssl.create_default_context()\n",
        "    with smtplib.SMTP(\"smtp.gmail.com\", 587, timeout=20) as server:\n",
        "        server.starttls(context=context)\n",
        "        server.login(EMAIL_USER, EMAIL_PASS)\n",
        "        server.send_message(msg)\n",
        "\n",
        "def frame_to_data_url(bgr, path=TMP_JPEG_PATH):\n",
        "    cv2.imwrite(path, bgr, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
        "    mime = mimetypes.guess_type(path)[0] or \"image/jpeg\"\n",
        "    with open(path, \"rb\") as f:\n",
        "        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "    return f\"data:{mime};base64,{b64}\"\n",
        "\n",
        "# =========================================================\n",
        "#               NODE: Prepare/Download video\n",
        "# =========================================================\n",
        "def prepare_video_node(state: AssistState) -> dict:\n",
        "    if IN_COLAB and not os.path.exists(state.input_video):\n",
        "        # sample video from blog\n",
        "        os.system(\"wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1nSFmlcLAiBiFfQkMEisdR5DMICcYSax0' -O /content/sample_detection.mp4\")\n",
        "        log(state, f\"Downloaded sample video to {state.input_video}\")\n",
        "    else:\n",
        "        log(state, f\"Using existing video: {state.input_video}\")\n",
        "    return {}\n",
        "\n",
        "\n",
        "def _build_clients():\n",
        "    openai_client = OpenAI(base_url=OPENAI_BASE_URL, api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n",
        "    groq_client   = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None\n",
        "    return openai_client, groq_client\n",
        "\n",
        "def ask_on_image_with_fallback(data_url: str, system_prompt: str, user_prompt: str):\n",
        "    \"\"\"\n",
        "    Try GPT-5 (OpenAI) first. On error/empty response, fall back to Groq.\n",
        "\n",
        "    \"\"\"\n",
        "    openai_client, groq_client = _build_clients()\n",
        "\n",
        "    # 1) OpenAI first\n",
        "    if openai_client:\n",
        "        try:\n",
        "            r = openai_client.chat.completions.create(\n",
        "                model=OPENAI_MODEL, temperature=0, max_tokens=8,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": user_prompt},\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
        "                    ]}\n",
        "                ]\n",
        "            )\n",
        "            txt = (r.choices[0].message.content or \"\").strip()\n",
        "            if txt:\n",
        "                return txt, \"OpenAI GPT-5\"\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 2) Fallback to Groq\n",
        "    if groq_client:\n",
        "        try:\n",
        "            r = groq_client.chat.completions.create(\n",
        "                model=GROQ_MODEL, temperature=0, max_tokens=8,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": user_prompt},\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
        "                    ]}\n",
        "                ]\n",
        "            )\n",
        "            txt = (r.choices[0].message.content or \"\").strip()\n",
        "            if txt:\n",
        "                return txt, \"Groq\"\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return \"\", \"none\"\n",
        "\n",
        "def chat_with_fallback(system_prompt: str, user_text: str, max_tokens: int = 64, temperature: float = 0.0):\n",
        "    \"\"\"\n",
        "    Text-only chat helper with fallback. Returns (reply_text, provider_name).\n",
        "    \"\"\"\n",
        "    openai_client, groq_client = _build_clients()\n",
        "\n",
        "    if openai_client:\n",
        "        try:\n",
        "            r = openai_client.chat.completions.create(\n",
        "                model=OPENAI_MODEL, temperature=temperature, max_tokens=max_tokens,\n",
        "                messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "                          {\"role\": \"user\",   \"content\": user_text}]\n",
        "            )\n",
        "            txt = (r.choices[0].message.content or \"\").strip()\n",
        "            if txt:\n",
        "                return txt, \"OpenAI GPT-5\"\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if groq_client:\n",
        "        try:\n",
        "            r = groq_client.chat.completions.create(\n",
        "                model=GROQ_MODEL, temperature=temperature, max_tokens=max_tokens,\n",
        "                messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "                          {\"role\": \"user\",   \"content\": user_text}]\n",
        "            )\n",
        "            txt = (r.choices[0].message.content or \"\").strip()\n",
        "            if txt:\n",
        "                return txt, \"OpenAI GPT-5\"\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return \"\", \"none\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Two-Stage Fall Detection Pipeline\n",
        "Node 1: Annotate/Track; Node 2: Groq Scan + Alerts"
      ],
      "metadata": {
        "id": "AO1E89gPz1o4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFJozlJHYaEY"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "#   NODE 1: Roboflow annotate + tracking + timers â†’ MP4\n",
        "# =========================================================\n",
        "def rf_annotate_node(state: AssistState) -> dict:\n",
        "    # Clean dirs\n",
        "    for d in (state.frames_dir, state.annotated_dir):\n",
        "        if os.path.exists(d): shutil.rmtree(d)\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "    # Extract frames\n",
        "    frames_gen = sv.get_video_frames_generator(state.input_video)\n",
        "    for i, frame in enumerate(frames_gen):\n",
        "        PILImage.fromarray(frame).save(f\"{state.frames_dir}/{i}.jpg\")\n",
        "    num_frames = len(os.listdir(state.frames_dir))\n",
        "    log(state, f\"ðŸ–¼ï¸ Saved {num_frames} frames to {state.frames_dir}\")\n",
        "\n",
        "    # Setup inference\n",
        "    client = InferenceHTTPClient(api_url=\"https://detect.roboflow.com\", api_key=ROBOFLOW_API_KEY)\n",
        "    BBOX_ANN = sv.BoundingBoxAnnotator()\n",
        "    LABEL_ANN = sv.LabelAnnotator()\n",
        "    tracker = sv.ByteTrack()\n",
        "    video_info = sv.VideoInfo.from_video_path(state.input_video)\n",
        "\n",
        "    class FPS_Timer:\n",
        "        def __init__(self, fps: int = 30) -> None:\n",
        "            self.fps = fps\n",
        "            self.frames_id = 0\n",
        "            self.tracker_id2frame_id: Dict[int, int] = {}\n",
        "        def reset_time(self, tracker_id: int) -> None:\n",
        "            self.tracker_id2frame_id[tracker_id] = self.frames_id\n",
        "        def reset_all(self):\n",
        "            for k in self.tracker_id2frame_id: self.tracker_id2frame_id[k] = self.frames_id\n",
        "        def tick(self, detections: sv.Detections) -> np.ndarray:\n",
        "            self.frames_id += 1\n",
        "            times = []\n",
        "            for tid in detections.tracker_id:\n",
        "                if tid not in self.tracker_id2frame_id:\n",
        "                    self.tracker_id2frame_id[tid] = self.frames_id\n",
        "                start = self.tracker_id2frame_id[tid]\n",
        "                times.append((self.frames_id - start) / self.fps)\n",
        "            return np.array(times)\n",
        "\n",
        "    timers = FPS_Timer(fps=video_info.fps)\n",
        "    last_status_by_id: Dict[int, str] = {}\n",
        "    prolonged_ids: set = set()\n",
        "\n",
        "    # Per-frame processing\n",
        "    for i in range(num_frames):\n",
        "        img_path = f\"{state.frames_dir}/{i}.jpg\"\n",
        "        frame_bgr = cv2.imread(img_path)\n",
        "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        result = client.infer(img_path, model_id=MODEL_ID)\n",
        "        detections = sv.Detections.from_inference(result)\n",
        "        detections = tracker.update_with_detections(detections)\n",
        "\n",
        "        class_names = detections.data.get(\"class_name\", [])\n",
        "        times = timers.tick(detections)\n",
        "\n",
        "        status_by_id, times_by_id = {}, {}\n",
        "        for idx, tid in enumerate(detections.tracker_id):\n",
        "            status = class_names[idx] if idx < len(class_names) else \"unknown\"\n",
        "            status_by_id[int(tid)] = status\n",
        "            times_by_id[int(tid)] = float(times[idx])\n",
        "            # Reset timer on status change\n",
        "            if last_status_by_id.get(int(tid)) != status:\n",
        "                timers.reset_time(int(tid))\n",
        "        last_status_by_id.update(status_by_id)\n",
        "\n",
        "        # Detect prolonged falls (model-based)\n",
        "        for tid, status in status_by_id.items():\n",
        "            if status == \"fallen\" and times_by_id.get(tid, 0.0) > FALL_SECONDS_THRESHOLD:\n",
        "                prolonged_ids.add(tid)\n",
        "\n",
        "        labels = [f\"{status_by_id[int(tid)]}: {times_by_id[int(tid)]:.1f}s\" for tid in detections.tracker_id]\n",
        "        annotated = BBOX_ANN.annotate(scene=frame_rgb.copy(), detections=detections)\n",
        "        annotated = LABEL_ANN.annotate(scene=annotated, detections=detections, labels=labels)\n",
        "        PILImage.fromarray(annotated).save(f\"{state.annotated_dir}/{i}.jpg\")\n",
        "\n",
        "    # Stitch to MP4\n",
        "    first = cv2.imread(os.path.join(state.annotated_dir, \"0.jpg\"))\n",
        "    h, w = first.shape[:2]\n",
        "    writer = cv2.VideoWriter(state.annotated_video, cv2.VideoWriter_fourcc(*\"mp4v\"), video_info.fps, (w, h))\n",
        "    for j in range(num_frames):\n",
        "        writer.write(cv2.imread(os.path.join(state.annotated_dir, f\"{j}.jpg\")))\n",
        "    writer.release()\n",
        "\n",
        "    state.num_frames = num_frames\n",
        "    state.video_fps = float(video_info.fps)\n",
        "    state.prolonged_fall_ids = sorted(list(prolonged_ids))\n",
        "    log(state, f\"âœ… Annotated video written to: {state.annotated_video}\")\n",
        "    if state.prolonged_fall_ids:\n",
        "        log(state, f\"[ALERT] Prolonged fallen IDs: {state.prolonged_fall_ids}\")\n",
        "    try:\n",
        "        display(Video(state.annotated_video, embed=True))\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {\"num_frames\": state.num_frames, \"video_fps\": state.video_fps, \"prolonged_fall_ids\": state.prolonged_fall_ids}\n",
        "\n",
        "# =========================================================\n",
        "#   NODE 2: Groq fall scan on annotated video â†’ voice + email\n",
        "# =========================================================\n",
        "# =========================================================\n",
        "#   NODE 2: GPT-5 fall scan on annotated video â†’ voice + email\n",
        "#            (minimal-diff: keeps state.groq_* fields)\n",
        "# =========================================================\n",
        "# ======== NODE 2: GPT-5 scan with Groq fallback â†’ voice + email + JSON ========\n",
        "# Minimal-diff: keeps state.groq_* fields and state.groq_report_path\n",
        "def groq_scan_node(state: AssistState) -> dict:\n",
        "    # Optional quick probe (won't throw if only Groq is present)\n",
        "    try:\n",
        "        if OPENAI_API_KEY:\n",
        "            OpenAI(base_url=OPENAI_BASE_URL, api_key=OPENAI_API_KEY).chat.completions.create(\n",
        "                model=OPENAI_MODEL, messages=[{\"role\": \"user\", \"content\": \"ping\"}], max_tokens=1\n",
        "            )\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    cap = cv2.VideoCapture(state.annotated_video)\n",
        "    assert cap.isOpened(), f\"Cannot open video: {state.annotated_video}\"\n",
        "    fps   = cap.get(cv2.CAP_PROP_FPS) or (state.video_fps or 30.0)\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    stride = max(1, int(round(fps * TARGET_SAMPLE_SEC)))\n",
        "\n",
        "    confirmed = False\n",
        "    first_detection: Optional[Tuple[int, float]] = None\n",
        "    frame_idx = 0\n",
        "    sample_idx = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while True:\n",
        "        ok, frame_bgr = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "\n",
        "        if frame_idx % stride == 0:\n",
        "            data_url = frame_to_data_url(frame_bgr, TMP_JPEG_PATH)\n",
        "            ans, provider = ask_on_image_with_fallback(data_url, SYSTEM_PROMPT, USER_PROMPT)\n",
        "            ts  = frame_idx / fps\n",
        "\n",
        "            # Only act/log if a fall is detected\n",
        "            if \"a person has fallen\" in ans.lower():\n",
        "                confirmed = True\n",
        "                first_detection = (frame_idx, ts)\n",
        "                cv2.imwrite(TRIGGER_FRAME_PATH, frame_bgr)\n",
        "                log(state, \"\\nðŸ›‘ FINAL VERDICT: FALL DETECTED\")\n",
        "                log(state, f\"ðŸ–¼ï¸ Saved trigger frame: {TRIGGER_FRAME_PATH}\")\n",
        "                speak_once(\"Alert. A fall has been detected.\")\n",
        "                try:\n",
        "                    subject = \"âš ï¸ Fall Alert Detected\"\n",
        "                    body = (\n",
        "                        \"A fall has been detected by the monitoring system.\\n\\n\"\n",
        "                        \"The trigger frame is attached.\"\n",
        "                    )\n",
        "                    send_email_alert(subject, body, CAREGIVER_EMAIL, attach_path=TRIGGER_FRAME_PATH)\n",
        "                    log(state, f\"ðŸ“§ Email sent to {CAREGIVER_EMAIL}\")\n",
        "                except smtplib.SMTPAuthenticationError as e:\n",
        "                    log(state, \"âŒ SMTP auth error. Use a Gmail App Password for EMAIL_PASS.\")\n",
        "                    log(state, f\"Details: {e}\")\n",
        "                except Exception as e:\n",
        "                    log(state, f\"âŒ Failed to send email: {e}\")\n",
        "                break\n",
        "\n",
        "            sample_idx += 1\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Report (keep original field names for compatibility)\n",
        "    state.groq_confirmed = confirmed\n",
        "    if confirmed and first_detection:\n",
        "        state.groq_first_detection_frame = int(first_detection[0])\n",
        "        state.groq_first_detection_time_s = float(first_detection[1])\n",
        "        state.trigger_frame_path = TRIGGER_FRAME_PATH\n",
        "    # (No \"NO FALL DETECTED\" log to stay silent unless positive)\n",
        "\n",
        "    report = {\n",
        "        \"input_video\": state.input_video,\n",
        "        \"annotated_video\": state.annotated_video,\n",
        "        \"model\": (OPENAI_MODEL if confirmed else OPENAI_MODEL),  # recorded key; provider is in email/logs\n",
        "        \"fps\": float(fps),\n",
        "        \"total_frames\": int(total),\n",
        "        \"stride_frames\": int(stride),\n",
        "        \"sample_interval_seconds\": round(stride / fps, 3),\n",
        "        \"first_detection\": (\n",
        "            {\"frame\": state.groq_first_detection_frame, \"time_s\": state.groq_first_detection_time_s}\n",
        "            if state.groq_first_detection_frame is not None else None\n",
        "        ),\n",
        "        \"trigger_frame_path\": state.trigger_frame_path if confirmed else None,\n",
        "        \"verdict\": \"fallen\" if confirmed else \"no_fall\",\n",
        "        \"elapsed_seconds\": round(time.time() - start_time, 2),\n",
        "        \"prolonged_fall_ids_model_stage\": state.prolonged_fall_ids,\n",
        "    }\n",
        "    with open(state.groq_report_path, \"w\") as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "    return {\n",
        "        \"groq_confirmed\": state.groq_confirmed,\n",
        "        \"groq_first_detection_frame\": state.groq_first_detection_frame,\n",
        "        \"groq_first_detection_time_s\": state.groq_first_detection_time_s,\n",
        "        \"trigger_frame_path\": state.trigger_frame_path,\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "#           NODE: Terminal (no-op, for clarity)\n",
        "# =========================================================\n",
        "def finalize_node(state: AssistState) -> dict:\n",
        "    log(state, \"âœ… Workflow complete.\")\n",
        "    return {}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building and Invoking Graph"
      ],
      "metadata": {
        "id": "UMgJlLi6z49h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJP-VYLXesKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "#                 BUILD THE AGENTIC GRAPH\n",
        "# =========================================================\n",
        "graph = StateGraph(AssistState)\n",
        "\n",
        "graph.add_node(\"prepare_video\", prepare_video_node)\n",
        "graph.add_node(\"rf_annotate\",   rf_annotate_node)\n",
        "graph.add_node(\"groq_scan\",     groq_scan_node)\n",
        "graph.add_node(\"finalize\",      finalize_node)\n",
        "\n",
        "# Linear flow: prepare â†’ annotate â†’ groq â†’ finalize\n",
        "graph.set_entry_point(\"prepare_video\")\n",
        "graph.add_edge(\"prepare_video\", \"rf_annotate\")\n",
        "graph.add_edge(\"rf_annotate\",   \"groq_scan\")\n",
        "graph.add_edge(\"groq_scan\",     \"finalize\")\n",
        "\n",
        "workflow = graph.compile()\n",
        "\n",
        "# =========================================================\n",
        "#                     RUN THE WORKFLOW\n",
        "# =========================================================\n",
        "state = AssistState()\n",
        "final_state = workflow.invoke(state)"
      ],
      "metadata": {
        "id": "RUpoo_yqyoj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y8y6vxkcx2PR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}